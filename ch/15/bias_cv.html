<!DOCTYPE html>
<html lang="en">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113006011-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113006011-1');
</script>


  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Cross Validation</title>
  <meta name="description" content="        Cross Validation        # HIDDENimport warnings# Ignore numpy dtype warnings. These warnings are caused by an interaction# between numpy and Cython a...">

  <link rel="canonical" href="https://cp71.github.io/dse_textbook/ch/15/bias_cv.html">
  <link rel="alternate" type="application/rss+xml" title="Principles and Techniques of Data Science" href="https://cp71.github.io/dse_textbook/feed.xml">

  <meta property="og:url"         content="https://cp71.github.io/dse_textbook/ch/15/bias_cv.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Cross Validation" />
<meta property="og:description" content="        Cross Validation        # HIDDENimport warnings# Ignore numpy dtype warnings. These warnings are caused by an interaction# between numpy and Cython a..." />
<meta property="og:image"       content="" />

<meta name="twitter:card" content="summary">


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage": "https://cp71.github.io/dse_textbook/ch/15/bias_cv.html",
  "headline": "Cross Validation",
  "datePublished": "2020-01-15T21:05:52+00:00",
  "dateModified": "2020-01-15T21:05:52+00:00",
  "description": "        Cross Validation        # HIDDENimport warnings# Ignore numpy dtype warnings. These warnings are caused by an interaction# between numpy and Cython a...",
  "author": {
    "@type": "Person",
    "name": "Sam Lau, Joey Gonzalez, and Deborah Nolan"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cp71.github.io/dse_textbook",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://cp71.github.io/dse_textbook",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/dse_textbook/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/dse_textbook/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<!-- (mostly) copied from nbconvert configuration -->
<!-- https://github.com/jupyter/nbconvert/blob/master/nbconvert/templates/html/mathjax.tpl -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true },
    }
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/dse_textbook';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js" async></script>
  <script>
const initAnchors = () => {
  if (window.anchors === undefined) {
    setTimeout(initAnchors, 250)
    return
  }
  anchors.add("main h1, main h2, main h3, main h4")
}

initFunction(initAnchors);
</script>


  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/turbolinks/5.2.0/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Selectors for elements on the page -->
  <script>
/**
 * Select various elements on the page for later use
 */

// IDs we'll attach to cells
const codeCellId = index => `codecell${index}`
const inputCellId = index => `inputcell${index}`

pageElements = {}

// All code cells
findCodeCells = function() {
    var codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre, div.text_cell_render div.highlight pre')
    pageElements['codeCells'] = codeCells;

    codeCells.forEach((codeCell, index) => {
      const id = codeCellId(index)
      codeCell.setAttribute('id', id)
    })
};

initFunction(findCodeCells);

// All cells in general
findInputCells = function() {
    var inputCells = document.querySelectorAll('div.jb_cell')
    pageElements['inputCells'] = inputCells;

    inputCells.forEach((inputCell, index) => {
        const id = inputCellId(index)
        inputCell.setAttribute('id', id)
    })
};

initFunction(findInputCells);
</script>

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->



  <!-- Load the auto-generating TOC -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js" async></script>
<script>
const initToc = () => {
  if (window.tocbot === undefined) {
    setTimeout(initToc, 250)
    return
  }

  // Check whether we have any sidebar content. If not, then show the sidebar earlier.
  var SIDEBAR_CONTENT_TAGS = ['.tag_full_width', '.tag_popout'];
  var sidebar_content_query = SIDEBAR_CONTENT_TAGS.join(', ')
  if (document.querySelectorAll(sidebar_content_query).length === 0) {
    document.querySelector('nav.onthispage').classList.add('no_sidebar_content')
  }

  // Initialize the TOC bot
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });

}
initFunction(initToc);
</script>


  <!-- Google analytics -->
  <script src="/dse_textbook/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script>

  <!-- Load custom website scripts -->
  <script src="/dse_textbook/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/dse_textbook/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/dse_textbook/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  

  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.6/lunr.min.js" async></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/dse_textbook/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  pageElements['codeCells'].forEach((codeCell) => {
    const id = codeCell.getAttribute('id')
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  <script>
    /**
    Add buttons to hide code cells
    */


    var setCodeCellVisibility = function (inputField, kind) {
        // Update the image and class for hidden
        var id = inputField.getAttribute('data-id');
        var codeCell = document.querySelector(`#${id} div.highlight`);

        if (kind === "visible") {
            codeCell.classList.remove('hidden');
            inputField.checked = true;
        } else {
            codeCell.classList.add('hidden');
            inputField.checked = false;
        }
    }

    var toggleCodeCellVisibility = function (event) {
        // The label is clicked, and now we decide what to do based on the input field's clicked status
        if (event.target.tagName === "LABEL") {
            var inputField = event.target.previousElementSibling;
        } else {
            // It is the span inside the target
            var inputField = event.target.parentElement.previousElementSibling;
        }

        if (inputField.checked === true) {
            setCodeCellVisibility(inputField, "visible");
        } else {
            setCodeCellVisibility(inputField, "hidden");
        }
    }


    // Button constructor
    const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

    var addHideButton = function () {
        // If a hide button is already added, don't add another
        if (document.querySelector('div.tag_hide_input input') !== null) {
            return;
        }

        // Find the input cells and add a hide button
        pageElements['inputCells'].forEach(function (inputCell) {
            if (!inputCell.classList.contains("tag_hide_input")) {
                // Skip the cell if it doesn't have a hidecode class
                return;
            }

            const id = inputCell.getAttribute('id')

            // Insert the button just inside the end of the next div
            inputCell.querySelector('div.input').insertAdjacentHTML('beforeend', hideCodeButton(id))

            // Set up the visibility toggle
            hideLink = document.querySelector(`#${id} div.inner_cell + input + label`);
            hideLink.addEventListener('click', toggleCodeCellVisibility)
        });
    }


    // Initialize the hide buttos
    var initHiddenCells = function () {
        // Add hide buttons to the cells
        addHideButton();

        // Toggle the code cells that should be hidden
        document.querySelectorAll('div.tag_hide_input input').forEach(function (item) {
            setCodeCellVisibility(item, 'hidden');
            item.checked = true;
        })
    }

    initFunction(initHiddenCells);

</script>

  <!-- Printing the screen -->
  <!-- Include nbinteract for interactive widgets -->
<script src="https://printjs-4de6.kxcdn.com/print.min.js" async></script>
<script>
printContent = () => {
    // MathJax displays a second version of any math for assistive devices etc.
    // This prevents double-rendering in the PDF output.
    var ignoreAssistList = [];
    assistives = document.querySelectorAll('.MathJax_Display span.MJX_Assistive_MathML').forEach((element, index) => {
        var thisId = 'MathJax-assistive-' + index.toString();
        element.setAttribute('id', thisId);
        ignoreAssistList.push(thisId)
    });

    // Print the actual content object
    printJS({
        printable: 'textbook_content',
        type: 'html',
        css: "/dse_textbook/assets/css/styles.css",
        scanStyles: false,
        targetStyles: ["*"],
        ignoreElements: ignoreAssistList
    })
};

initPrint = () => {
    document.querySelector('#interact-button-print').addEventListener('click', printContent)
}

initFunction(initPrint)
</script>

</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      <nav id="js-sidebar" class="c-textbook__sidebar"><h2 class="c-sidebar__title">Principles and Techniques of Data Science</h2>
  <ul class="c-sidebar__chapters"><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="http://ds100.org/"
        >Data 100 Homepage
        </a></li>
        <li class="c-sidebar__chapter">
          <a class="c-sidebar__entry"
            href="/dse_textbook/search.html"
          >
            Search This Book
          </a>
        </li>
        <li class="c-sidebar__divider"></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/intro.html"
        >Introduction
        </a></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/about_this_book.html"
        >About This Book
        </a></li><li class="c-sidebar__divider"></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/01/lifecycle_intro.html"
        >1. The Data Science Lifecycle
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/01/lifecycle_students_1.html"
                >1.1 The Students of Data 100
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/01/lifecycle_students_2.html"
                >1.2 Exploring the Data
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/01/lifecycle_students_3.html"
                >1.3 What's in a Name?
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/02/design_intro.html"
        >2. Data Design
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/02/design_dewey_truman.html"
                >2.1 Dewey Defeats Truman
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/02/design_prob_overview.html"
                >2.2 Probability Overview
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/02/design_sampling.html"
                >2.3 Probability Sampling
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/02/design_srs_vs_big_data.html"
                >2.4 SRS vs. "Big Data"
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/03/pandas_intro.html"
        >3. Tabular Data and pandas
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/03/pandas_structure.html"
                >3.1 Structure
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/03/pandas_indexes.html"
                >3.2 Indexes, Slicing, and Sorting
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/03/pandas_grouping_pivoting.html"
                >3.3 Grouping and Pivoting
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/03/pandas_apply_strings_plotting.html"
                >3.4 Apply, Strings, and Plotting
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/04/eda_intro.html"
        >4. Exploratory Data Analysis
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/04/eda_data_types.html"
                >4.1 Data Types
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href=".html"
                >4.2 Distributions [in progress]
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href=".html"
                >4.3 Associations [in progress]
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/05/cleaning_intro.html"
        >5. Data Cleaning
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/05/cleaning_calls.html"
                >5.1 Cleaning the Calls Dataset
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/05/cleaning_stops.html"
                >5.2 Cleaning the Stops Dataset
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/05/cleaning_structure.html"
                >5.3 Structure and Joins
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/05/cleaning_granularity.html"
                >5.4 Granularity
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/05/cleaning_scope.html"
                >5.5 Scope
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/05/cleaning_temp.html"
                >5.6 Temporality
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/05/cleaning_faithfulness.html"
                >5.7 Faithfulness
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/06/viz_intro.html"
        >6. Data Visualization
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/06/viz_quantitative.html"
                >6.1 Quantitative Data
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/06/viz_qualitative.html"
                >6.2 Qualitative Data
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/06/viz_matplotlib.html"
                >6.3 Customizing Plots
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/06/viz_principles.html"
                >6.4 Principles of Visualization
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/06/viz_principles_2.html"
                >6.5 Principles of Visualization 2
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/06/viz_philosophy.html"
                >6.6 Visualization Philosophy
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/07/web_intro.html"
        >7. Web Technologies
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/07/web_http.html"
                >7.1 HTTP
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/08/text_intro.html"
        >8. Working With Text
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/08/text_strings.html"
                >8.1 Python String Methods
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/08/text_regex.html"
                >8.2 Regular Expressions
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/08/text_re.html"
                >8.3 Regex in Python and pandas
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/09/sql_intro.html"
        >9. Databases and SQL
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/09/sql_rdbms.html"
                >9.1 Relational Databases
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/09/sql_basics.html"
                >9.2 SQL Queries
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/09/sql_joins.html"
                >9.3 SQL Joins
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/10/modeling_intro.html"
        >10. Modeling and Estimation
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/10/modeling_simple.html"
                >10.1 A Simple Model
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/10/modeling_loss_functions.html"
                >10.2 Loss Functions
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/10/modeling_abs_huber.html"
                >10.3 Absolute Cost and Huber Cost
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/11/gradient_descent.html"
        >11. Gradient Descent
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/11/gradient_basics.html"
                >11.1 Basic Numerical Optimization
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/11/gradient_descent_define.html"
                >11.2 Defining Gradient Descent
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/11/gradient_convexity.html"
                >11.3 Convexity
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/11/gradient_stochastic.html"
                >11.4 Stochastic Gradient Descent
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/12/prob_and_gen.html"
        >12. Probability and Generalization
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/12/prob_random_vars.html"
                >12.1 Random Variables
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/12/prob_exp_var.html"
                >12.2 Expectation and Variance
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/12/prob_risk.html"
                >12.3 Risk
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/13/linear_models.html"
        >13. Linear Regression
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/13/linear_tips.html"
                >13.1 Defining a Simple Linear Model
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/13/linear_grad.html"
                >13.2 Fitting the Model
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/13/linear_multiple.html"
                >13.3 Multiple Linear Regression
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/13/linear_projection.html"
                >13.4 A Geometric Perspective
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/13/linear_case_study.html"
                >13.5 Linear Regression Case Study
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/14/feature_engineering.html"
        >14. Feature Engineering
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/14/feature_one_hot.html"
                >14.1 One-Hot Encoding
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/14/feature_polynomial.html"
                >14.2 Polynomial Regression
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/15/bias_intro.html"
        >15. Bias-Variance Tradeoff
        </a><ul class="c-sidebar__sections "><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/15/bias_risk.html"
                >15.1 Risk and Cost Minimization
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/15/bias_modeling.html"
                >15.2 Model Bias and Variance
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry c-sidebar__entry--active"
                  href="/dse_textbook/ch/15/bias_cv.html"
                >15.3 Cross Validation
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/16/reg_intro.html"
        >16. Regularization
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/16/reg_intuition.html"
                >16.1 Regularization Intuition
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/16/reg_ridge.html"
                >16.2 L2 Regularization
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/16/reg_lasso.html"
                >16.3 L1 Regularization
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/17/classification_intro.html"
        >17. Classification
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_prob.html"
                >17.1 Regression on Probabilities
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_log_model.html"
                >17.2 Logistic Model
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_cost.html"
                >17.3 Cross-Entropy Loss
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_log_reg.html"
                >17.4 Using Logistic Regression
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_cost_justification.html"
                >17.5 Justifying Cross-Entropy Loss
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_sgd.html"
                >17.6 Fitting a Logistic Model
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_sensitivity_specificity.html"
                >17.7 Evaluating Logistic Models
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/17/classification_multiclass.html"
                >17.8 Multiclass Classification
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/18/hyp_intro.html"
        >18. Statistical Inference
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/18/hyp_introduction.html"
                >18.1 Introduction to Hypothesis Testing
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/18/hyp_introduction_part2.html"
                >18.2 Permutation Testing
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/18/hyp_regression.html"
                >18.3 Bootstrapping for Linear Regression
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/18/hyp_studentized.html"
                >18.4 Studentized Bootstrap
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/18/hyp_phacking.html"
                >18.5 P-Hacking
                </a></li></ul></li><li class="c-sidebar__divider"></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/19/vector_space_review.html"
        >Appendix: Vector Space Review
        </a></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/20/ref_intro.html"
        >Appendix: Reference Tables
        </a><ul class="c-sidebar__sections u-hidden-visually"><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/20/ref_pandas.html"
                >pandas
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/20/ref_seaborn.html"
                >seaborn
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/20/ref_matplotlib.html"
                >matplotlib
                </a></li><li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/dse_textbook/ch/20/ref_sklearn.html"
                >scikit-learn
                </a></li></ul></li><li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/dse_textbook/ch/21/contributors.html"
        >Appendix: Contributors
        </a></li></ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <div class="c-topbar" id="top-navbar">
  <!-- We show the sidebar by default so we use .is-active -->
  <div class="c-topbar__buttons">
    <button
      id="js-sidebar-toggle"
      class="hamburger hamburger--arrowalt is-active"
    >
      <span class="hamburger-box">
        <span class="hamburger-inner"></span>
      </span>
    </button>
    <div class="buttons">
<div class="download-buttons-dropdown">
    <button id="dropdown-button-trigger" class="interact-button"><i class="fa fa-download"></i></button>
    <div class="download-buttons">
        <a href="/dse_textbook/content/ch/15/bias_cv.ipynb" download>
        <button id="interact-button-download" class="interact-button">ORIG</button>
        </a>
        
        <a id="interact-button-print"><button id="interact-button-download" class="interact-button">PDF</button></a>
    </div>
</div>

  
  
  
  


</div>

  </div>
  <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
  <aside class="sidebar__right">
    <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
    <nav class="onthispage">
    </nav>
  </aside>
  <a href="/dse_textbook/search.html" class="topbar-right-button" id="search-button"><i class="fa fa-search"></i></a>
</div>

      <main class="c-textbook__page" tabindex="-1">
            <div class="c-textbook__content" id="textbook_content">
                  <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Cross Validation</div>
</div>
    <div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="c1"># Ignore numpy dtype warnings. These warnings are caused by an interaction</span>
<span class="c1"># between numpy and Cython and can be safely ignored.</span>
<span class="c1"># Reference: https://stackoverflow.com/a/40846742</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.dtype size changed&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.ufunc size changed&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">nbinteract</span> <span class="k">as</span> <span class="nn">nbi</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># This option stops scientific notation for pandas</span>
<span class="c1"># pd.set_option(&#39;display.float_format&#39;, &#39;{:.2f}&#39;.format)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="k">def</span> <span class="nf">df_interact</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Outputs sliders that show rows and columns of df</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">peek</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row</span><span class="p">:</span><span class="n">row</span> <span class="o">+</span> <span class="n">nrows</span><span class="p">,</span> <span class="n">col</span><span class="p">:</span><span class="n">col</span> <span class="o">+</span> <span class="n">ncols</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">ncols</span><span class="p">:</span>
        <span class="n">interact</span><span class="p">(</span><span class="n">peek</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">-</span> <span class="n">nrows</span><span class="p">,</span> <span class="n">nrows</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="n">fixed</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">interact</span><span class="p">(</span><span class="n">peek</span><span class="p">,</span>
                 <span class="n">row</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">-</span> <span class="n">nrows</span><span class="p">,</span> <span class="n">nrows</span><span class="p">),</span>
                 <span class="n">col</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">-</span> <span class="n">ncols</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(</span><span class="si">{}</span><span class="s1"> rows, </span><span class="si">{}</span><span class="s1"> columns) total&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cross-Validation">Cross-Validation<a class="anchor-link" href="#Cross-Validation"> </a></h2><p>In the previous section, we observed that we needed a more accurate way of simulating the test error to manage the bias-variance trade off. To reiterate, training error is misleadingly low, because we are fitting our model on the training set. We need to choose a model without using the test set, so we split our training set again, into a validation set. Cross-validation provides a method of estimating our model error using a single observed dataset by separating data used for training from the data used for model selection and final accuracy.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-Validation-Test-Split">Train-Validation-Test Split<a class="anchor-link" href="#Train-Validation-Test-Split"> </a></h2><p>One way to accomplish this is to split the original dataset into three disjoint subsets:</p>
<ul>
<li>Training set: The data used to fit the model.</li>
<li>Validation set: The data used to select features.</li>
<li>Test set: The data used to report the model's final accuracy.</li>
</ul>
<p>After splitting, we select a set of features and a model based on the following procedure:</p>
<ol>
<li>For each potential set of features, fit a model using the training set. The error of a model on the training set is its <em>training error</em>.</li>
<li>Check the error of each model on the validation set: its <em>validation error</em>. Select the model that achieves the lowest validation error. This is the final choice of features and model.</li>
<li>Calculate the <em>test error</em>, error of the final model on the test set. This is the final reported accuracy of the model. We are forbidden from adjusting the features or model to decrease test error; doing so effectively converts the test set into a validation set. Instead, we must collect a new test set after making further changes to the features or the model.</li>
</ol>
<p>This process allows us to more accurately determine the model to use than using the training error alone. By using cross-validation, we can test our model on data that it wasn't fit on, simulating test error without using the test set. This gives us a sense of how our model performs on unseen data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Size of the train-validation-test split</strong></p>
<p>The train-validation-test split commonly uses 70% of the data as the training set, 15% as the validation set, and the remaining 15% as the test set. Increasing the size of the training set helps model accuracy but causes more variation in the validation and test error. This is because a smaller validation set and test set are less representative of the sample data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-Error-and-Test-Error">Training Error and Test Error<a class="anchor-link" href="#Training-Error-and-Test-Error"> </a></h2><p>A model is of little use to us if it fails to generalize to unseen data from the population. The test error provides the most accurate representation of the model's performance on new data since we do not use the test set to train the model or select features.</p>
<p>In general, the training error decreases as we add complexity to our model with additional features or more complex prediction mechanisms. The test error, on the other hand, decreases up to a certain amount of complexity then increases again as the model overfits the training set. This is due the fact that at first, bias decreases more than variance increases. Eventually, the increase in variance surpasses the decrease in bias. 
<img src="https://raw.githubusercontent.com/DS-100/textbook/master/assets/feature_train_test_error.png" alt="feature_train_test_error.png"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="K-Fold-Cross-Validation">K-Fold Cross-Validation<a class="anchor-link" href="#K-Fold-Cross-Validation"> </a></h2><p>The <strong>train-validation-test split</strong> method is a good method to simulate test error through the validation set. However, making the three splits results in too little data for training. Also, with this method the validation error may be prone to high variance because the evaluation of the error may depend heavily on which points end up in the training and validation sets.</p>
<p>To tackle this problem, we can run the train-validation split multiple times on the same dataset. The dataset is divided into <em>k</em> equally-sized subsets (<em>$k$ folds</em>), and the train-validation split is repeated <em>k</em> times. Each time, one of the <em>k</em> folds is used as the validation set, and the remaining <em>k-1</em> folds are used as the training set. We report the model's final validation error as the average of the $ k $ validation errors from each trial. This method is called <strong>k-fold cross-validation</strong>.</p>
<p>The diagram below illustrates the technique when using five folds:</p>
<p><img src="https://github.com/DS-100/textbook/blob/master/assets/feature_5_fold_cv.jpg?raw=true" alt="feature_5_fold_cv.jpg"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The biggest advantage of this method is that every data point is used for validation exactly once and for training <em>k-1</em> times. Typically, a <em>k</em> between 5 to 10 is used, but <em>k</em> remains an unfixed parameter. When <em>k</em> is small, the error estimate has a lower variance (many validation points) but has a higher bias (fewer training points). Vice versa, with large <em>k</em> the error estimate has lower bias but has higher variance.</p>
<p>$k$-fold cross-validation takes more computation time than the train-validation split since we typically have to refit each model from scratch for each fold. However, it computes a more accurate validation error by averaging multiple errors together for each model.</p>
<p>The <code>scikit-learn</code> library provides a convenient <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"><code>sklearn.model_selection.KFold</code></a> class to implement $k$-fold cross-validation.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bias-Variance-Tradeoff">Bias-Variance Tradeoff<a class="anchor-link" href="#Bias-Variance-Tradeoff"> </a></h2><p>Cross-validation helps us manage the bias-variance tradeoff more accurately. Intuitively, the validation error estimates test error by checking the model's performance on a dataset not used for training; this allows us to estimate both model bias and model variance. K-fold cross-validation also incorporates the fact that the noise in the test set only affects the noise term in the bias-variance decomposition whereas the noise in the training set affects both bias and model variance. To choose the final model to use, we select the one that has the lowest validation error.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example:-Model-Selection-for-Ice-Cream-Ratings">Example: Model Selection for Ice Cream Ratings<a class="anchor-link" href="#Example:-Model-Selection-for-Ice-Cream-Ratings"> </a></h2><p>We will use the complete model selection process, including cross-validation, to select a model that predicts ice cream ratings from ice cream sweetness. The complete ice cream dataset and a scatter plot of the overall rating versus ice cream sweetness are shown below.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">ice</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;icecream.csv&#39;</span><span class="p">)</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">ice</span><span class="p">[[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">]])</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ice</span><span class="p">[[</span><span class="s1">&#39;overall&#39;</span><span class="p">]])</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">12.5</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">rating_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span>

<span class="n">temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">])</span>
<span class="n">temp</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rating_pred</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x_devs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))</span>
<span class="n">y_devs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))</span>
<span class="n">temp</span><span class="p">[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">temp</span><span class="p">[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_devs</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">temp</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">temp</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">y_devs</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ice</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">temp</span><span class="p">,</span> <span class="n">ice</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ice</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sweetness</th>
      <th>overall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.60</td>
      <td>3.09</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.50</td>
      <td>3.17</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.69</td>
      <td>3.46</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>11.00</td>
      <td>5.90</td>
    </tr>
    <tr>
      <th>7</th>
      <td>11.70</td>
      <td>5.50</td>
    </tr>
    <tr>
      <th>8</th>
      <td>11.90</td>
      <td>5.40</td>
    </tr>
  </tbody>
</table>
<p>309 rows × 2 columns</p>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ice</span><span class="p">[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">],</span> <span class="n">ice</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Ice Cream Rating vs. Sweetness&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sweetness&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Rating&#39;</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/ch/15/bias_cv_12_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using degree 10 polynomial features on 9 random points from the dataset result in a perfectly accurate model for those data points. Unfortunately, this model fails to generalize to previously unseen data from the population.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">ice2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;icecream.csv&#39;</span><span class="p">)</span>
<span class="n">trans_ten</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_ten</span> <span class="o">=</span> <span class="n">trans_ten</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">ice2</span><span class="p">[[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ice2</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">]</span>

<span class="n">clf_ten</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_ten</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_devs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ice2</span><span class="p">))</span>
<span class="n">y_devs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ice2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ice2</span><span class="p">[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">],</span> <span class="n">ice2</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">])</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">12.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">clf_ten</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">trans_ten</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Degree 10 polynomial fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">clf_ten</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">trans_ten</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ice2</span><span class="p">[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_devs</span><span class="p">,</span>
            <span class="n">ice2</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">y_devs</span><span class="p">,</span>
            <span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Degree 10 poly, second set of data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/ch/15/bias_cv_15_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Instead of the above method, we first partition our data into training, validation, and test datasets using <code>scikit-learn</code>'s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code>sklearn.model_selection.train_test_split</code></a> method to perform a 70/30% train-test split.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">test_size</span> <span class="o">=</span> <span class="mi">92</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">ice</span><span class="p">[[</span><span class="s1">&#39;sweetness&#39;</span><span class="p">]],</span> <span class="n">ice</span><span class="p">[</span><span class="s1">&#39;overall&#39;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;  Training set size: {len(X_train)}&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;      Test set size: {len(X_test)}&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  Training set size: 217
      Test set size: 92
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now fit polynomial regression models using the training set, one for each polynomial degree from 1 to 10.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="c1"># First, we add polynomial features to X_train</span>
<span class="n">transformers</span> <span class="o">=</span> <span class="p">[</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">deg</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">deg</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)]</span>
<span class="n">X_train_polys</span> <span class="o">=</span> <span class="p">[</span><span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
                 <span class="k">for</span> <span class="n">transformer</span> <span class="ow">in</span> <span class="n">transformers</span><span class="p">]</span>

<span class="c1"># Display the X_train with degree 5 polynomial features</span>
<span class="n">X_train_polys</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[     1.  ,      8.8 ,     77.44,    681.47,   5996.95,  52773.19],
       [     1.  ,     10.74,    115.35,   1238.83,  13305.07, 142896.44],
       [     1.  ,      9.98,     99.6 ,    994.01,   9920.24,  99003.99],
       ...,
       [     1.  ,      6.79,     46.1 ,    313.05,   2125.59,  14432.74],
       [     1.  ,      5.13,     26.32,    135.01,    692.58,   3552.93],
       [     1.  ,      8.66,     75.  ,    649.46,   5624.34,  48706.78]])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will then perform 5-fold cross-validation on the 10 featurized datasets. To do so, we will define a function that:</p>
<ol>
<li>Uses the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"><code>KFold.split</code></a> function to get 5 splits on the training data. Note that <code>split</code> returns the indices of the data for that split.</li>
<li>For each split, select out the rows and columns based on the split indices and features.</li>
<li>Fit a linear model on the training split.</li>
<li>Compute the mean squared error on the validation split.</li>
<li>Return the average error across all cross validation splits.</li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="k">def</span> <span class="nf">mse_cost</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_actual</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_actual</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_CV_error</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">):</span>
    <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">validation_errors</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">valid_idx</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_train</span><span class="p">):</span>
        <span class="c1"># split the data</span>
        <span class="n">split_X_train</span><span class="p">,</span> <span class="n">split_X_valid</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">valid_idx</span><span class="p">]</span>
        <span class="n">split_Y_train</span><span class="p">,</span> <span class="n">split_Y_valid</span> <span class="o">=</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">valid_idx</span><span class="p">]</span>

        <span class="c1"># Fit the model on the training split</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">split_X_train</span><span class="p">,</span><span class="n">split_Y_train</span><span class="p">)</span>
        
        <span class="c1"># Compute the RMSE on the validation split</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">mse_cost</span><span class="p">(</span><span class="n">split_Y_valid</span><span class="p">,</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">split_X_valid</span><span class="p">))</span>
        
        <span class="n">validation_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    
    <span class="c1">#average validation errors</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">validation_errors</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># We train a linear regression classifier for each featurized dataset and perform cross-validation</span>
<span class="c1"># We set fit_intercept=False for our linear regression classifier since </span>
<span class="c1"># the PolynomialFeatures transformer adds the bias column for us.</span>

<span class="n">cross_validation_errors</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_CV_error</span><span class="p">(</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">X_train_poly</span> <span class="ow">in</span> <span class="n">X_train_polys</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">cv_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Validation Error&#39;</span><span class="p">:</span> <span class="n">cross_validation_errors</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="n">cv_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Degree&#39;</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">display</span><span class="p">(</span><span class="n">cv_df</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Validation Error</th>
    </tr>
    <tr>
      <th>Degree</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.324820</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.045060</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.045418</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.045282</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.046272</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.046715</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.047140</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.047540</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.048055</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.047805</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that as we use higher degree polynomial features, the validation error decreases and increases again.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_hide_input">

<div class="cell border-box-sizing code_cell rendered tag_hide_input">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">cv_df</span><span class="p">[</span><span class="s1">&#39;Validation Error&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">cv_df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">cv_df</span><span class="p">[</span><span class="s1">&#39;Validation Error&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Validation Error vs. Polynomial Degree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Polynomial Degree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Validation Error&#39;</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">cv_df</span><span class="p">[</span><span class="s1">&#39;Validation Error&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">cv_df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">cv_df</span><span class="p">[</span><span class="s1">&#39;Validation Error&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.044925</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Zoomed In&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Polynomial Degree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Validation Error&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/ch/15/bias_cv_25_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Examining the validation errors reveals that the most accurate model only used degree 2 polynomial features. Thus, we select the degree 2 polynomial model as our final model and fit it on the all of the training data at once. Then, we compute its error on the test set.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_trans</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_polys</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">training_error</span> <span class="o">=</span> <span class="n">mse_cost</span><span class="p">(</span><span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_polys</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">validation_error</span> <span class="o">=</span> <span class="n">cross_validation_errors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mse_cost</span><span class="p">(</span><span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">best_trans</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)),</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Degree 2 polynomial&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;  Training error: </span><span class="si">{training_error:0.5f}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Validation error: </span><span class="si">{validation_error:0.5f}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;      Test error: </span><span class="si">{test_error:0.5f}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Degree 2 polynomial
  Training error: 0.04409
Validation error: 0.04506
      Test error: 0.04698
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For future reference, <code>scikit-learn</code> has a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html"><code>cross_val_predict</code></a> method to automatically perform cross-validation, so we don't have to break the data into training and validation sets ourselves.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also, note that the test error is higher than the validation error which is higher than the training error. The training error should be the lowest because the model is fit on the training data. Fitting the model minimizes the mean squared error for that dataset. The validation error and the test error are usually higher than the training error because the error is computed on an unknown dataset that the model hasn't seen.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary"> </a></h2><p>We use the widely useful cross-validation technique to manage the bias-variance tradeoff. After computing a train-validation-test split on the original dataset, we use the following procedure to train and choose a model.</p>
<ol>
<li>For each potential set of features, fit a model using the training set. The error of a model on the training set is its <em>training error</em>.</li>
<li>Check the error of each model on the validation set using $k$-fold cross-validation: its <em>validation error</em>. Select the model that achieves the lowest validation error. This is the final choice of features and model.</li>
<li>Calculate the <em>test error</em>, error of the final model on the test set. This is the final reported accuracy of the model. We are forbidden from adjusting the model to increase test error; doing so effectively converts the test set into a validation set. Instead, we must collect a new test set after making further changes to the model.</li>
</ol>

</div>
</div>
</div>
</div>

 


    </main>
    
            </div>
            <nav class="c-page__nav">
  
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/dse_textbook/ch/15/bias_modeling.html">
      〈 <span class="u-margin-right-tiny"></span> 
    </a>
  

  
    <a id="js-page__nav__next" class="c-page__nav__next" href="/dse_textbook/ch/16/reg_intro.html">
       <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

            <footer>
  <p class="footer"></p>
</footer>

        </div>
      </main>
    </div>
  </body>
</html>
